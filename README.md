# Exploring Musical Emotion Visualization Through Acoustic Feature Mapping

## Abstract
This project explores whether the emotional qualities of music can be communicated visually by mapping the set of acustic features, Energy (RMS), Spectral Centroid and Tempo (BPM), to simple dynamic visual parameters such as movement intensity, color and rhythmic pulsing. This is explored using four short instrumental excerpts, each of which conveys an emotion (happiness, sadness). The system perform feature extraction and generates visuals whose temporal evolution is related with the set of acustic features. A user study compares valence and arousal ratings accross three different tests, audio, visual, and audio+visual, with the goal to evaluate how these visuals approximate or modify the emotional impressions of the music. By examining how visual changes generated from acoustic features evoke emotional responses, the project aims to explore, on a small scale, how musical emotion can be experienced through sight, with potential implications for users with hearing impairments.

---
## Introduction
Music is widely recognized as a powerful medium capable of conveying emotions. Although the emotions generated by music are mainly transmitted through sound, people often describe the experience using visual imagery such as colors, shapes, movements or atmospheres inspired by the music (Dahl et al., 2023). This observation suggests the existence of significant relationships between acoustic structure and visual representation, opening the possibility to investigate how music can be experienced through visual channels.

Unfortunately, the emotions conveyed by music are not accessible to everyone. For people with hearing impairments, these characteristics are difficult or impossible to experience directly through sound. Therefore, creating visual representations that aim to reflect these properties may offer an alternative way to enjoy the emotional qualities of music. However, to reach this point, it is necessary to understand how these aspects of sound can be related and coherently rendered in a visual form. 

To address this challenge, it is essential to identify which musical features are most relevant for the perception of emotions, as well as to determine how these features can be expressed visually. Rather than focusing on the aesthetic design of the visuals, the project approaches the problem from a perceptual perspective.

The project aims to explain and apply correspondences between acustic and visual features supported by previous research. The goal is not to replicate the richness and complexity of music in visual form, but rather to examine how changes in energy (RMS), spectral centroid and tempo (BPM) can, through variations in movement, color and rhythmic pulsing, evoke affective responses that aling with those produced by music.

Finally, a user study compares emotional responses across three conditions (audio, visual and audio+visual), using Russell's valence-arousal model (Russel 1980). The study focuses on two regions of the affective space, as it examines short musical excerpts categorized specifically as expressing happiness and sadness.

---
## Theoretical Framework and Related Studies
- Musical emotion and the valence-arousal model (Russell, 1980)
- Acoustic features and their role in emotional perception (Haumann, 2018, 2021): Describe relationships between acoustic features and emotion and justify my selection of three features.
- Music evoked mental imagery (Dahl, 2023): Sometimes listeners saw mental visual imagery when hearing music. Imagery patterns often correspond to acoustic properties.
- Relationships between sound and vision (Spence 2011, Palmer 2013, Marks 1975, Tresilian 2020): justification of why visuals can evoke similar emotions, how humans match diferent acustic features to visual changes. Justification of my visual features choice and their mapping to an acustic feature.
- Short paragraph summarizing and leading into the Research Question and Hypotheses.

---
## Research Question & Hypotheses
Can visuals generated from Energy (RMS), Spectral Centroid and Tempo (BPM) evoke emotional impressions similar to those experienced during music listening through changes in movement, color and rhythmic pulsing?

- H1: It is expected that the emotional responses elicited by the visual only test will fall within the same quadrant (or similar) of the valence-arousal space as the responses elicited by the audio only test. (Independent variable: Test modality) (Dependent variables: valence and arousal)
- H2: When audio and visuals are presented together, it is expected to produce enhanced emotional responses than with each individual condition. (Independent variable: Test modality) (Dependent variables: valence and arousal)

---
## Methodology

---
## Notes for me:
(TRACTAR DE SER MOLT METICULOS AL TEST I *INVESTIGAR COM PREGUNTAR ALS USUARIS LES SEVES EMOCIONS*)

Per a que els usuaris sapiguen com categoritzar les emocions mitjançant els visuals, els dos primers tests que relitzen poden ser audio+visual d'una canço trista y altra feliç, y despres de forma aleatoria que es presente nomes la part visual (d'altra canço) y que tracten de categoritzar "l'emoció" que esta transmetent la peça.

---
### References

---

*Authored by Joan Montés Mora - MSc in Medialogy | Sound & Music Perception and Cognition course 2025*
